"""
LLM service for memory extraction and personalization.
"""

import logging
import json
from typing import List, Dict, Any, Optional
from datetime import datetime, timezone
from config.settings import get_settings

logger = logging.getLogger(__name__)
settings = get_settings()


class LLMService:
    """Service for LLM operations."""

    def __init__(self, organization_id: Optional[str] = None):
        """
        Initialize LLM service.
        
        Args:
            organization_id: Optional organization ID for per-organization provider selection
        """
        # For now, use global setting (per-organization config can be added later)
        self.provider = settings.llm_provider
        self.api_key = settings.llm_api_key
        self.model = settings.llm_model
        self.timeout = settings.llm_timeout_seconds
        self.organization_id = organization_id

    async def extract_memories(
        self,
        transcript: List[Dict[str, str]],
        agent_id: str,
        caller_id: str,
        conversation_id: str
    ) -> List[Dict[str, Any]]:
        """
        Extract memories from conversation transcript using LLM.

        Args:
            transcript: List of message objects with role and message
            agent_id: Agent identifier
            caller_id: Caller identifier
            conversation_id: Conversation identifier

        Returns:
            List of extracted memory objects
        """
        try:
            # Pre-filter transcript to remove system prompts, config, and bad content
            filtered_transcript = self._filter_transcript(transcript)
            
            if not filtered_transcript:
                logger.warning(f"No valid conversation messages found in transcript for {conversation_id}")
                return []
            
            logger.info(
                f"Filtered transcript: {len(transcript)} -> {len(filtered_transcript)} messages "
                f"for conversation {conversation_id}"
            )
            
            # Chunk long conversations with overlap and metadata
            chunk_data_list = self._chunk_transcript(
                filtered_transcript,
                max_tokens=settings.llm_max_tokens_per_chunk,
                overlap_tokens=settings.llm_chunk_overlap_tokens,
                conversation_id=conversation_id
            )
            
            all_memories = []
            failed_chunks = []
            chunk_stats = {
                "total_chunks": len(chunk_data_list),
                "successful_chunks": 0,
                "failed_chunks": 0,
                "total_memories_extracted": 0
            }
            
            # Process each chunk independently with error isolation
            for chunk_data in chunk_data_list:
                chunk = chunk_data["chunk"]
                chunk_metadata = chunk_data["metadata"]
                chunk_index = chunk_metadata["chunk_index"]
                total_chunks = chunk_metadata["total_chunks"]
                
                try:
                    logger.info(
                        f"Processing chunk {chunk_index + 1}/{total_chunks} "
                        f"for conversation {conversation_id} "
                        f"({chunk_metadata['token_count']} tokens, "
                        f"{chunk_metadata['message_count']} messages)"
                    )
                    
                    # Check if we need json_object format (for GPT-4 models)
                    use_json_object = "gpt-4" in self.model.lower()
                    
                    prompt = self._create_memory_extraction_prompt(
                        chunk, agent_id, caller_id, conversation_id, use_json_object=use_json_object
                    )

                    # Select provider (with fallback support)
                    selected_provider = self._select_provider()
                    
                    # Try primary provider with fallback
                    chunk_memories = await self._extract_with_fallback(prompt, selected_provider)
                    
                    # Validate response
                    validated_chunk_memories = self._validate_memory_response(chunk_memories)
                    
                    # Apply privacy filters
                    from .privacy import get_privacy_filter
                    privacy_filter = get_privacy_filter()
                    
                    filtered_memories = []
                    for memory in validated_chunk_memories:
                        if privacy_filter.should_redact_memory(memory):
                            # Redact sensitive information
                            content = memory.get("content", "")
                            memory["content"] = privacy_filter.filter_sensitive_info(content)
                            memory["metadata"] = memory.get("metadata", {})
                            memory["metadata"]["privacy_filtered"] = True
                            logger.info("Applied privacy filter to memory")
                        filtered_memories.append(memory)
                    
                    all_memories.extend(filtered_memories)
                    chunk_stats["successful_chunks"] += 1
                    chunk_stats["total_memories_extracted"] += len(filtered_memories)
                    
                    logger.info(
                        f"Successfully processed chunk {chunk_index + 1}/{total_chunks}: "
                        f"extracted {len(filtered_memories)} memories"
                    )
                    
                except Exception as e:
                    # Error isolation: log failure but continue processing other chunks
                    logger.error(
                        f"Error processing chunk {chunk_index + 1}/{total_chunks} "
                        f"for conversation {conversation_id}: {e}",
                        exc_info=True
                    )
                    failed_chunks.append({
                        "chunk_index": chunk_index,
                        "error": str(e),
                        "metadata": chunk_metadata
                    })
                    chunk_stats["failed_chunks"] += 1
            
            logger.info(
                f"Memory extraction complete for conversation {conversation_id}: "
                f"{chunk_stats['successful_chunks']}/{chunk_stats['total_chunks']} chunks successful, "
                f"{chunk_stats['total_memories_extracted']} memories extracted"
            )
            
            if failed_chunks:
                logger.warning(
                    f"Failed to process {len(failed_chunks)} chunks for conversation {conversation_id}. "
                    f"Chunks: {[c['chunk_index'] for c in failed_chunks]}"
                )
            
            return all_memories

        except Exception as e:
            logger.error(f"Error extracting memories: {e}", exc_info=True)
            return []
    
    def _select_provider(self) -> str:
        """
        Select LLM provider based on configuration.
        
        Returns:
            Provider name (openai or anthropic)
        """
        if self.provider == "auto":
            # Auto-select: prefer OpenAI if available, fallback to Anthropic
            # For now, default to OpenAI (can be enhanced with availability checks)
            logger.info("Auto provider selection: using OpenAI")
            return "openai"
        elif self.provider in ["openai", "anthropic"]:
            return self.provider
        else:
            logger.warning(f"Unknown provider {self.provider}, defaulting to OpenAI")
            return "openai"
    
    async def _extract_with_fallback(
        self,
        prompt: str,
        primary_provider: str
    ) -> List[Dict[str, Any]]:
        """
        Extract memories with automatic fallback to alternative provider on failure.
        
        Args:
            prompt: Extraction prompt
            primary_provider: Primary provider to try first
        
        Returns:
            List of extracted memories
        """
        # Determine fallback provider
        fallback_provider = "anthropic" if primary_provider == "openai" else "openai"
        
        # Try primary provider
        try:
            if primary_provider == "openai":
                memories = await self._extract_with_openai(prompt)
            else:
                memories = await self._extract_with_anthropic(prompt)
            
            if memories:
                logger.info(f"Successfully extracted memories using {primary_provider}")
                return memories
        except Exception as e:
            logger.warning(
                f"Primary provider {primary_provider} failed: {e}. "
                f"Falling back to {fallback_provider}"
            )
        
        # Try fallback provider
        try:
            if fallback_provider == "openai":
                memories = await self._extract_with_openai(prompt)
            else:
                memories = await self._extract_with_anthropic(prompt)
            
            if memories:
                logger.info(f"Successfully extracted memories using fallback {fallback_provider}")
                return memories
        except Exception as e:
            logger.error(f"Fallback provider {fallback_provider} also failed: {e}")
        
        return []
    
    def _validate_memory_response(
        self,
        memories: List[Dict[str, Any]]
    ) -> List[Dict[str, Any]]:
        """
        Validate memory response using Pydantic models.
        
        Args:
            memories: Raw memory list from LLM
        
        Returns:
            Validated memory list
        """
        from pydantic import ValidationError
        
        validated = []
        valid_categories = ["factual", "preference", "issue", "emotional", "relational"]
        
        for memory in memories:
            try:
                # Validate required fields
                if not isinstance(memory, dict):
                    logger.warning(f"Invalid memory format (not dict): {memory}")
                    continue
                
                content = memory.get("content", "")
                category = memory.get("category", "factual")
                importance = memory.get("importance", 5)
                entities = memory.get("entities", [])
                
                # Validate content
                if not content or not isinstance(content, str):
                    logger.warning(f"Invalid memory content: {content}")
                    continue
                
                # Validate category
                if category not in valid_categories:
                    logger.warning(f"Invalid category {category}, defaulting to factual")
                    category = "factual"
                
                # Validate importance (1-10)
                if not isinstance(importance, int) or importance < 1 or importance > 10:
                    logger.warning(f"Invalid importance {importance}, defaulting to 5")
                    importance = 5
                
                # Validate entities
                if not isinstance(entities, list):
                    entities = []
                
                validated.append({
                    "content": content,
                    "category": category,
                    "importance": importance,
                    "entities": entities
                })
                
            except Exception as e:
                logger.warning(f"Error validating memory: {e}, skipping")
                continue
        
        return validated

    def _estimate_tokens(self, text: str) -> int:
        """
        Estimate token count (rough approximation: 1 token â‰ˆ 4 characters).
        
        Args:
            text: Text to estimate
        
        Returns:
            Estimated token count
        """
        return len(text) // 4
    
    def _chunk_transcript(
        self,
        transcript: List[Dict[str, str]],
        max_tokens: Optional[int] = None,
        overlap_tokens: Optional[int] = None,
        conversation_id: Optional[str] = None
    ) -> List[Dict[str, Any]]:
        """
        Chunk long conversations for memory extraction with overlap and boundary respect.
        
        Args:
            transcript: Full transcript
            max_tokens: Maximum tokens per chunk (defaults to settings.llm_max_tokens_per_chunk)
            overlap_tokens: Overlap tokens between chunks (defaults to settings.llm_chunk_overlap_tokens)
            conversation_id: Conversation ID for chunk metadata
        
        Returns:
            List of chunk dictionaries with metadata: [{"chunk": [...], "metadata": {...}}, ...]
        """
        if not transcript:
            return []
        
        # Use settings defaults if not provided
        if max_tokens is None:
            max_tokens = settings.llm_max_tokens_per_chunk
        if overlap_tokens is None:
            overlap_tokens = settings.llm_chunk_overlap_tokens
        
        # Estimate total tokens
        full_text = "\n".join([
            f"{msg.get('role', 'unknown')}: {msg.get('message', '')}"
            for msg in transcript
        ])
        total_tokens = self._estimate_tokens(full_text)
        
        # If under limit, return as single chunk with metadata
        if total_tokens <= max_tokens:
            return [{
                "chunk": transcript,
                "metadata": {
                    "chunk_index": 0,
                    "total_chunks": 1,
                    "conversation_id": conversation_id,
                    "token_count": total_tokens,
                    "message_count": len(transcript)
                }
            }]
        
        # Split into chunks with overlap and boundary respect
        chunks = []
        current_chunk = []
        current_tokens = 0
        chunk_index = 0
        overlap_messages = []
        overlap_tokens_count = 0
        
        i = 0
        while i < len(transcript):
            msg = transcript[i]
            msg_text = f"{msg.get('role', 'unknown')}: {msg.get('message', '')}"
            msg_tokens = self._estimate_tokens(msg_text)
            
            # Check if adding this message would exceed limit
            if current_tokens + msg_tokens > max_tokens and current_chunk:
                # Try to find a good boundary (prefer user/agent transitions)
                # Look ahead for a better split point
                split_at = i
                if i < len(transcript) - 1:
                    # Prefer splitting after agent messages or before user messages
                    current_role = msg.get('role', '')
                    # If current message is agent, include it and split after
                    # If current message is user, try to split before it
                    if current_role == 'user' and len(current_chunk) > 0:
                        # Split before this user message
                        split_at = i
                    else:
                        # Include current message and split after
                        split_at = i + 1
                
                # Save current chunk
                chunks.append({
                    "chunk": current_chunk.copy(),
                    "metadata": {
                        "chunk_index": chunk_index,
                        "total_chunks": 0,  # Will update later
                        "conversation_id": conversation_id,
                        "token_count": current_tokens,
                        "message_count": len(current_chunk)
                    }
                })
                
                # Prepare overlap for next chunk
                # Take last N messages that fit within overlap_tokens
                overlap_messages = []
                overlap_tokens_count = 0
                for j in range(len(current_chunk) - 1, -1, -1):
                    overlap_msg = current_chunk[j]
                    overlap_msg_text = f"{overlap_msg.get('role', 'unknown')}: {overlap_msg.get('message', '')}"
                    overlap_msg_tokens = self._estimate_tokens(overlap_msg_text)
                    if overlap_tokens_count + overlap_msg_tokens <= overlap_tokens:
                        overlap_messages.insert(0, overlap_msg)
                        overlap_tokens_count += overlap_msg_tokens
                    else:
                        break
                
                # Start new chunk with overlap
                current_chunk = overlap_messages.copy()
                current_tokens = overlap_tokens_count
                chunk_index += 1
                
                # Add current message if we didn't split before it
                if split_at > i:
                    current_chunk.append(msg)
                    current_tokens += msg_tokens
                    i += 1
            else:
                current_chunk.append(msg)
                current_tokens += msg_tokens
                i += 1
        
        # Add final chunk
        if current_chunk:
            chunks.append({
                "chunk": current_chunk,
                "metadata": {
                    "chunk_index": chunk_index,
                    "total_chunks": len(chunks) + 1,
                    "conversation_id": conversation_id,
                    "token_count": current_tokens,
                    "message_count": len(current_chunk)
                }
            })
        
        # Update total_chunks in all metadata
        total_chunks = len(chunks)
        for chunk_data in chunks:
            chunk_data["metadata"]["total_chunks"] = total_chunks
        
        logger.info(
            f"Split transcript into {total_chunks} chunks "
            f"(total tokens: {total_tokens}, overlap: {overlap_tokens} tokens per chunk)"
        )
        
        return chunks

    def _filter_transcript(self, transcript: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        Filter transcript to remove system prompts, config, and bad content.
        
        Args:
            transcript: Raw transcript with all message fields
            
        Returns:
            Filtered transcript with only clean conversation messages
        """
        import re
        
        filtered = []
        system_keywords = [
            "system", "config", "configuration", "api", "endpoint",
            "json", "yaml", "xml", "markdown", "code", "function",
            "tool_call", "rag_retrieval", "llm_usage", "metadata"
        ]
        
        json_pattern = re.compile(r'\{[^{}]*"[^{}]*"[^{}]*\}', re.IGNORECASE)
        config_pattern = re.compile(r'(max_|min_|timeout|config|setting)', re.IGNORECASE)
        
        for msg in transcript:
            role = msg.get("role", "").lower()
            message = msg.get("message", "")
            
            # Skip system messages
            if role == "system":
                continue
            
            # Only process user and agent messages
            if role not in ["user", "agent"]:
                continue
            
            # Skip empty or very short messages
            if not message or len(message.strip()) < 3:
                continue
            
            # Skip messages that look like JSON/config
            message_lower = message.lower()
            
            # Check for JSON-like structures
            if json_pattern.search(message) and len(message) < 500:
                # Short JSON snippets are likely config, skip them
                continue
            
            # Check for system/config keywords in short messages
            if len(message) < 200:
                has_system_keywords = any(keyword in message_lower for keyword in system_keywords)
                if has_system_keywords and config_pattern.search(message):
                    continue
            
            # Check for markdown table patterns (system documentation)
            if "|" in message and "---" in message and message.count("|") > 3:
                # Likely a markdown table, skip
                continue
            
            # Check for code-like patterns
            if message.count("{") > 2 and message.count("}") > 2:
                # Multiple braces suggest JSON/config structure
                if len(message) < 300:
                    continue
            
            # Keep the message but only include essential fields
            clean_msg = {
                "role": role,
                "message": message
            }
            filtered.append(clean_msg)
        
        return filtered

    def _create_memory_extraction_prompt(
        self,
        transcript: List[Dict[str, str]],
        agent_id: str,
        caller_id: str,
        conversation_id: str,
        use_json_object: bool = False
    ) -> str:
        """Create prompt for memory extraction."""

        # Format transcript
        transcript_text = "\n".join([
            f"{msg.get('role', 'unknown')}: {msg.get('message', '')}"
            for msg in transcript
        ])

        timestamp = datetime.now(timezone.utc).isoformat()

        # Format response based on whether we're using json_object format
        if use_json_object:
            response_format = """Return ONLY a JSON object (no markdown, no explanation):
{{
  "memories": [
    {{
      "content": "Clear, concise, atomic memory statement",
      "category": "factual|preference|issue|emotional|relational",
      "importance": 1-10,
      "entities": ["entity1", "entity2"]
    }}
  ]
}}"""
        else:
            response_format = """Return ONLY a JSON array (no markdown, no explanation):
[
  {{
    "content": "Clear, concise, atomic memory statement",
    "category": "factual|preference|issue|emotional|relational",
    "importance": 1-10,
    "entities": ["entity1", "entity2"]
  }}
]"""

        return f"""Extract memories from this AI agent conversation for future reference.

Conversation Details:
- Agent ID: {agent_id}
- Caller ID: {caller_id}
- Conversation ID: {conversation_id}
- Timestamp: {timestamp}

Full Transcript:
{transcript_text}

Extract memories in these categories:
1. FACTUAL: Names (FIRST AND LAST names separately), IDs, numbers, dates, locations, transactions, objective facts
2. PREFERENCES: User preferences, likes/dislikes, communication style, scheduling preferences
3. ISSUES: Problems mentioned, complaints, unresolved issues, follow-up needed
4. EMOTIONAL: Customer sentiment (satisfied, frustrated, neutral), tone of interaction
5. RELATIONAL: People or entities mentioned, relationships between concepts

{response_format}

Rules:
- Each memory should be ONE atomic fact
- Be specific and factual
- IMPORTANT: Extract BOTH first name AND last name as SEPARATE memories if both are mentioned
- Names are high importance (8-9): Extract "Caller's first name is [name]" and "Caller's last name is [name]" separately
- If a name is spelled out (e.g., "3-B-R-E-E-T"), extract it as the actual name (e.g., "Breet")
- Importance: 10=critical (account numbers, VIP status), 8-9=names, 1=minor detail
- Extract 5-20 memories per conversation
- If nothing memorable, return empty array [] or {{"memories": []}}

CRITICAL CONTENT FILTERING RULES:
- ONLY extract from actual spoken conversation between user and agent
- SKIP any content that appears to be:
  * System configuration or settings
  * API documentation or endpoints
  * JSON structures, YAML, XML, or code snippets
  * Markdown tables or technical documentation
  * Agent instructions or system prompts
  * Configuration parameters (max_, min_, timeout, etc.)
  * Tool calls, RAG retrieval info, or LLM usage data
  * Metadata or technical implementation details
- ONLY extract factual information, preferences, issues, emotions, or relationships mentioned in the conversation
- If content looks like technical documentation, configuration, or system data, skip it entirely
- Focus on what the USER said or what the AGENT said about the user, not system internals
"""

    async def _extract_with_openai(self, prompt: str) -> List[Dict[str, Any]]:
        """Extract memories using OpenAI API."""
        try:
            from openai import AsyncOpenAI
            import httpx

            client = AsyncOpenAI(
                api_key=self.api_key,
                timeout=httpx.Timeout(self.timeout, connect=10.0)
            )

            response = await client.chat.completions.create(
                model=self.model,
                messages=[{"role": "user", "content": prompt}],
                temperature=0.3,
                response_format={"type": "json_object"} if "gpt-4" in self.model else None,
                timeout=self.timeout
            )

            content = response.choices[0].message.content.strip()

            # Try to parse JSON
            try:
                # Handle markdown code blocks
                if content.startswith("```"):
                    content = content.split("```")[1]
                    if content.startswith("json"):
                        content = content[4:]
                    content = content.strip()

                # Parse JSON
                result = json.loads(content)

                # Handle different response formats
                if isinstance(result, list):
                    return result
                elif isinstance(result, dict):
                    # Check for common wrapper keys
                    for key in ["memories", "results", "data"]:
                        if key in result and isinstance(result[key], list):
                            return result[key]
                    return [result]
                else:
                    logger.error(f"Unexpected result type: {type(result)}")
                    return []

            except json.JSONDecodeError as e:
                logger.error(f"Failed to parse JSON response: {e}\nContent: {content}")
                return []

        except Exception as e:
            logger.error(f"Error with OpenAI API: {e}", exc_info=True)
            return []

    async def _extract_with_anthropic(self, prompt: str) -> List[Dict[str, Any]]:
        """Extract memories using Anthropic API."""
        try:
            import anthropic

            client = anthropic.AsyncAnthropic(api_key=self.api_key)

            response = await client.messages.create(
                model=self.model,
                max_tokens=2000,
                temperature=0.3,
                messages=[{"role": "user", "content": prompt}],
                timeout=self.timeout
            )

            content = response.content[0].text.strip()

            # Parse JSON response
            try:
                if content.startswith("```"):
                    content = content.split("```")[1]
                    if content.startswith("json"):
                        content = content[4:]
                    content = content.strip()

                result = json.loads(content)

                if isinstance(result, list):
                    return result
                elif isinstance(result, dict):
                    for key in ["memories", "results", "data"]:
                        if key in result and isinstance(result[key], list):
                            return result[key]
                    return [result]
                else:
                    return []

            except json.JSONDecodeError as e:
                logger.error(f"Failed to parse JSON response: {e}")
                return []

        except Exception as e:
            logger.error(f"Error with Anthropic API: {e}", exc_info=True)
            return []

    async def generate_personalized_first_message(
        self,
        agent_profile: Dict[str, Any],
        last_conv_memories: List[Dict[str, Any]],
        cross_agent_memories: List[Dict[str, Any]]
    ) -> str:
        """
        Generate personalized first message for returning caller.

        Args:
            agent_profile: Agent profile from ElevenLabs
            last_conv_memories: Memories from last conversation
            cross_agent_memories: High-importance memories from other agents

        Returns:
            Personalized first message
        """
        try:
            # If no previous memories, use default
            if not last_conv_memories:
                default_msg = agent_profile.get("first_message", "Hello! How can I help you today?")
                logger.info("No previous memories, using default first message")
                return default_msg

            prompt = self._create_first_message_prompt(
                agent_profile, last_conv_memories, cross_agent_memories
            )

            # Call LLM
            if self.provider == "openai":
                first_message = await self._generate_with_openai(prompt)
            elif self.provider == "anthropic":
                first_message = await self._generate_with_anthropic(prompt)
            else:
                logger.error(f"Unsupported LLM provider: {self.provider}")
                return agent_profile.get("first_message", "Hello!")

            logger.info("Generated personalized first message")
            return first_message

        except Exception as e:
            logger.error(f"Error generating first message: {e}", exc_info=True)
            return agent_profile.get("first_message", "Hello! How can I help you today?")

    def _create_first_message_prompt(
        self,
        agent_profile: Dict[str, Any],
        last_conv_memories: List[Dict[str, Any]],
        cross_agent_memories: List[Dict[str, Any]]
    ) -> str:
        """Create prompt for first message personalization."""

        # Format last conversation memories
        last_conv_text = "\n".join([
            f"- {m.get('content', '')}"
            for m in last_conv_memories
        ]) if last_conv_memories else "None"

        # Format cross-agent memories
        cross_agent_text = "\n".join([
            f"- {m.get('content', '')} (importance: {m.get('metadata', {}).get('importance', 0)})"
            for m in cross_agent_memories
        ]) if cross_agent_memories else "None"

        agent_name = agent_profile.get("name", "AI Assistant")
        default_first_message = agent_profile.get("first_message", "Hello!")
        system_prompt = agent_profile.get("system_prompt", "")[:200]

        return f"""Personalize the first message for an AI agent calling back a customer.

Agent Info:
- Name: {agent_name}
- Default First Message: {default_first_message}
- System Prompt Summary: {system_prompt}...

Caller's Last Conversation:
{last_conv_text}

High-Priority Context from Other Interactions:
{cross_agent_text}

Task: Generate a warm, personalized first message (20-40 words) that:
1. Naturally acknowledges previous interactions
2. References relevant context without overwhelming the caller
3. Maintains the agent's tone and purpose
4. Feels conversational and human, not robotic

Return ONLY the first message text (no JSON, no explanation).
"""

    async def _generate_with_openai(self, prompt: str) -> str:
        """Generate first message using OpenAI."""
        try:
            from openai import AsyncOpenAI

            client = AsyncOpenAI(api_key=self.api_key)

            response = await client.chat.completions.create(
                model=self.model,
                messages=[{"role": "user", "content": prompt}],
                temperature=0.7,
                max_tokens=100
            )

            return response.choices[0].message.content.strip()

        except Exception as e:
            logger.error(f"Error with OpenAI API: {e}", exc_info=True)
            return ""

    async def _generate_with_anthropic(self, prompt: str) -> str:
        """Generate first message using Anthropic."""
        try:
            import anthropic

            client = anthropic.AsyncAnthropic(api_key=self.api_key)

            response = await client.messages.create(
                model=self.model,
                max_tokens=100,
                temperature=0.7,
                messages=[{"role": "user", "content": prompt}]
            )

            return response.content[0].text.strip()

        except Exception as e:
            logger.error(f"Error with Anthropic API: {e}", exc_info=True)
            return ""
